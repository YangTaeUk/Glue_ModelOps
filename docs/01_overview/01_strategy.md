# 1. Strategic Positioning

> 왜 이 프로젝트인가?

## 핵심 철학

**우리는 "AI 모델 개발사"가 아니라 "AI 서비스 제공사"이다.**

- 경쟁력: "최신 모델을 얼마나 빨리 튜닝하느냐"가 아님
- 경쟁력: **"어떤 모델이든 얼마나 안정적으로 서비스에 녹여내느냐"**

> **팀장 코멘트**: "우리는 '기술적 깊이(Deep Tech)'보다 '운영적 넓이(Operational Width)'를 커버하는 Foundation 구축에 집중한다."

---

## 문제 정의

| 문제 | 현상 | 결과 |
|------|------|------|
| 중복 개발 | 모델마다 API, 인증, 큐 재구축 | 시간 낭비, 품질 불균형 |
| 인프라 복잡도 | KServe/Istio 무거운 스택 | 운영 부담, 디버깅 어려움 |
| 비용 | SageMaker 의존 | 월 $30,000+ |
| 전문성 분산 | 개발자가 모델+인프라 담당 | 핵심 역량 희석 |

## 해결 방향

> **Variable(모델)과 Invariant(인프라)를 분리한다.**

- 모델: 계속 변함 (새 모델, 업그레이드)
- 인프라: 변하지 않음 (비동기, 보안, 모니터링)

→ 인프라를 한 번 잘 만들면 어떤 모델이든 즉시 서비스 가능

## 포지셔닝

```
┌─────────────────────────────────────────────────────┐
│                    기존 솔루션                        │
├─────────────────────────────────────────────────────┤
│  SageMaker    │  비쌈, 벤더 종속                     │
│  KServe       │  복잡함, Istio 의존성                │
│  직접 개발     │  중복, 품질 불균형                   │
└─────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────┐
│              우리 솔루션: Thin Wrapper               │
├─────────────────────────────────────────────────────┤
│  • 추론 엔진(vLLM, Triton)을 감싸는 얇은 레이어      │
│  • 엔진 내부 수정 없음                               │
│  • 표준 인터페이스만 제공                            │
└─────────────────────────────────────────────────────┘
```

---

## 비교 분석: 왜 이 접근인가?

### vs 특화 라이브러리 (vLLM, Triton, TGI)

vLLM이나 Triton은 훌륭한 **'엔진(Engine)'**이지만, **'자동차(Car)'**는 아니다.

| 비교 항목 | 특화 라이브러리 | 우리 시스템 | Why |
|-----------|----------------|-------------|-----|
| 핵심 역할 | 빠른 추론 연산 | 비즈니스 로직, 상태 관리 | 엔진은 빠르지만, 요청 실패 시 그냥 죽음 |
| 요청 대기열 | 단순 FIFO (메모리) | Priority Queue, 영속 상태 | 서버 재시작 시 요청 보존 필요 |
| 보안/인증 | Basic Auth 수준 | RBAC, PII 마스킹, Rate Limit | vLLM 직접 노출 = 보안 사고 |
| 확장성 | 수동 복제 | 오토스케일링 정책 | 트래픽 몰릴 때 누가 연결해주나? |

> **결론**: 특화 라이브러리는 **'가져다 쓰는 대상'**이지 경쟁 대상이 아니다. 우리는 이 엔진들을 **'감싸는(Wrap)'** 안정적인 껍질을 만든다.

### vs 대규모 플랫폼 (KServe, SageMaker)

KServe나 SageMaker는 훌륭하지만, 소규모 팀에겐 **오버엔지니어링**의 위험이 크다.

| 비교 항목 | 대규모 플랫폼 | 우리 시스템 | Why |
|-----------|-------------|-------------|-----|
| 구축 난이도 | 최상 (Istio, Knative, DNS 등) | 중 (FastAPI + Temporal) | 우리 팀 규모로 K8s 네트워크까지 관리 불가 |
| 비용 | 높음 (Managed 비용/오버헤드) | 최적화 가능 (Spot 등) | SageMaker 비용은 상상 초월 |
| 유연성 | 프레임워크 규칙 준수 | 코드 레벨 자유 수정 | "RAG 중간에 외부 API 호출" → Temporal 코드 몇 줄 |
| 디버깅 | YAML 지옥, 로그 분산 | Python 코드 + Temporal UI | 에러 위치 즉시 확인 가능 |

> **결론**: KServe는 '인프라'로 문제를 풀고, 우리는 **'코드(Temporal)'**로 문제를 푼다. 인원이 적을수록 인프라 복잡도를 낮추고 코드 제어권을 확보하는 것이 유리하다.

---

## Core Competency

### 우리가 하는 것
| 영역 | 구현 |
|------|------|
| Workflow Orchestration | Temporal |
| Universal Interface | OpenAI API Compatible |
| Gatekeeping | Rate Limit, Auth, Quota |

### 우리가 안 하는 것
| 영역 | 담당 |
|------|------|
| 추론 엔진 최적화 | vLLM, Triton |
| GPU 저수준 관리 | Kubernetes |
| Service Mesh | Nginx로 충분 |

## 기대 효과

| 지표 | Before | After |
|------|--------|-------|
| 새 모델 배포 | 2-4주 | 1-2일 |
| 인프라 코드 중복 | 모델당 100% | 0% |
| 운영 복잡도 | 모델당 관리 | 통합 관리 |

---

## 의사결정 매트릭스 (팀원 가이드)

| 상황 | 행동 지침 |
|------|----------|
| **새로운 모델 추가** | 그 모델을 지원하는 Docker 이미지(vLLM 등)가 있는지 먼저 찾는다. 직접 Python 코드는 최후 수단. |
| **추론 속도 느림** | Python 코드 튜닝 X → TensorRT 변환 또는 엔진 설정(Batch Size) 튜닝 |
| **서버 다운 빈발** | 서버 코드 수정 전에 Temporal Retry 정책 + K8s Liveness Probe 먼저 확인 |
| **복잡한 파이프라인** | KServe YAML 작성 X → Temporal Workflow 코드로 해결 |

---

## Related

- [02_srs.md](./02_srs.md) - 시스템 요건 정의서
- [ADR-001: Gateway 선택](../99_decisions/001_gateway_nginx.md)
- [ADR-002: 통신 프로토콜](../99_decisions/002_communication_protocol.md)
- [ADR-003: Thin Wrapper 패턴](../99_decisions/003_thin_wrapper.md)
