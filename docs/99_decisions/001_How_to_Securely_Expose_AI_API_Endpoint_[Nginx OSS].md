# ADR-001: AI API 엔드포인트를 외부에 안전하게 노출하는 방법

> **Status**: Accepted
> **Date**: 2024-12-24
> **Deciders**: AI Serving Foundation Team

---

## TL;DR (한 줄 요약)

> AI API를 외부에 안전하게 노출해야 하는 상황에서, 보안/캐시/TLS 등 최소 요구사항을 효율적으로 충족하기 위해 **리버스 프록시**를 도입하기로 했다. 후보군(Nginx, Traefik) 중 **Nginx OSS**를 선택했다. LLM 스트리밍 튜닝의 용이성과 팀 역량이 결정적 이유였다.

---

## 1. 문제 정의 (The Problem)

### 1.1 배경 (Context)

**이 논의가 왜 시작되었는가?**

AI 모델 서빙 시스템을 구축하면서, 이 시스템이 **외부에 노출**되어야 하는 상황이 발생했다.

#### 1.1.1 일반적인 외부 노출 요구사항

시스템을 외부에 노출할 때는 다음과 같은 **최소 요구사항**이 존재한다:

| 요구사항 | 설명 |
|----------|------|
| **보안** | 인증되지 않은 접근 차단, DDoS 방어, IP 제한 |
| **TLS 종료** | HTTPS 연결 처리, 인증서 관리 |
| **캐시** | 반복 요청에 대한 응답 캐싱으로 백엔드 부하 감소 |
| **라우팅** | 요청을 적절한 백엔드 서비스로 전달 |
| **Rate Limiting** | 과도한 요청 제한, 서비스 보호 |
| **로깅/모니터링** | 요청 추적, 장애 대응을 위한 가시성 확보 |

이 모든 요구사항을 **각각 따로 구현**하는 것은 비효율적이다.
**한 레이어에서 통합 처리**하는 방법이 필요했다.

#### 1.1.2 AI API의 특수성 — 일반 웹 API와의 차이

그런데 우리가 노출해야 하는 것은 **일반 REST API가 아닌 AI/LLM API**이다.
AI API는 일반 웹 API와 근본적으로 다른 특성을 가진다:

| 특성 | 일반 REST API | AI/LLM API |
|------|---------------|------------|
| **응답 시간** | 밀리초 단위 | 수 초 ~ 수 분 (추론 시간) |
| **응답 방식** | 단일 응답 (JSON) | 스트리밍 (SSE, `text/event-stream`) |
| **연결 유지** | 짧은 연결 | 긴 연결 유지 (`keep-alive`) |
| **타임아웃** | 기본값 (30초) 충분 | 긴 타임아웃 필요 (수 분) |
| **버퍼링** | 버퍼링 가능 | 버퍼링 비활성화 필요 (실시간 토큰 전달) |
| **리소스 사용** | 가벼움 | GPU 전체 점유 가능 |

**참고 자료:**
- [vLLM 공식 문서 - Using Nginx](https://docs.vllm.ai/en/stable/deployment/nginx.html): vLLM은 프로덕션 배포 시 Nginx를 프론트에 두는 것을 권장
- [How streaming LLM APIs work](https://til.simonwillison.net/llms/streaming-llm-apis): LLM API는 `text/event-stream` 헤더로 토큰을 스트리밍
- [AWS API Gateway 스트리밍 지원](https://aws.amazon.com/blogs/compute/building-responsive-apis-with-amazon-api-gateway-response-streaming/): LLM 워크로드를 위해 15분 타임아웃까지 지원 확대
- [API7 - API Gateway가 LLM 요청을 프록시하는 방법](https://api7.ai/learning-center/api-gateway-guide/api-gateway-proxy-llm-requests): SSE 스트리밍 보존, 토큰 기반 Rate Limiting, 지능형 재시도 필요

이러한 특수성 때문에 **아무 프록시나 선택할 수 없다**.
AI API 스트리밍을 제대로 지원하는 프록시가 필요하다.

### 1.2 핵심 문제 (Core Issue)

**겉으로 드러난 현상 vs 본질적 원인:**

| 현상 (Symptom) | 본질 (Root Cause) |
|---------------|-------------------|
| "AI API를 외부에 어떻게 열어야 하지?" | 외부 노출 시 필수적인 보안/성능/운영 요구사항을 효율적으로 충족하는 방법이 필요함 |
| "보안, 캐시, TLS를 각각 어디서 처리하지?" | 여러 횡단 관심사(cross-cutting concerns)를 한 곳에서 통합 관리해야 함 |
| "LLM 응답이 끊기거나 타임아웃이 발생한다" | 일반 API용 기본 설정으로는 AI 스트리밍을 처리할 수 없음 |
| "토큰이 한꺼번에 오거나 지연된다" | 프록시 버퍼링이 실시간 스트리밍을 방해함 |

**문제의 범위와 경계:**

- **포함**: TLS 종료, 라우팅, Rate Limit, CORS, 기본 보안, **SSE 스트리밍 지원**, **긴 타임아웃 처리**
- **미포함**: 비즈니스 로직 인증/인가 (FastAPI에서 처리), 모델 추론

### 1.3 해결책의 방향

이 문제를 해결하는 가장 효율적인 방법은 **리버스 프록시**를 두는 것이다.

**왜 리버스 프록시인가?**

```
[Client] → [Reverse Proxy] → [Backend (FastAPI)]
              ↑
     보안, TLS, 캐시, 라우팅,
     Rate Limit, 로깅 통합 처리
```

- 백엔드(FastAPI)는 비즈니스 로직에만 집중
- 횡단 관심사는 프록시 레이어에서 일괄 처리
- 백엔드 변경 없이 정책 수정 가능

### 1.4 비즈니스 임팩트 (Business Impact)

**적절한 외부 노출 방법을 선택하지 않을 경우:**

| 영역 | 영향 |
|------|------|
| **보안** | API 직접 노출 → 공격 표면 증가, 취약점 노출 |
| **성능** | 캐시/압축 미적용 → 불필요한 백엔드 부하 |
| **운영** | 설정 분산 → 일관성 부재, 디버깅 어려움 |
| **확장** | 백엔드 종속적 설정 → 스케일아웃 시 복잡도 증가 |

**올바르게 해결했을 때:**

- 보안/성능/운영 요구사항을 한 레이어에서 통합 관리
- 백엔드 독립성 유지 → 유연한 확장
- 정책 변경이 프록시 설정만으로 완료

---

## 2. 리서치 및 제약 사항 (Constraints & Facts)

(다음 섹션에서 계속...)

---

## 3. 사고 과정 및 대안 분석 (Analysis & Reasoning)

(다음 섹션에서 계속...)

---

## 4. 최종 결정 및 실행 계획 (Decision & Action)

(다음 섹션에서 계속...)
