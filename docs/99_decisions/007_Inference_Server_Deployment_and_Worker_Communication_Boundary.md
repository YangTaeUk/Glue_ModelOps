# ADR-007: 추론 서버 배포와 Worker 통신 경계 전략

> **Status**: Accepted
> **Date**: 2024-12-27
> **Author**: System Architect
> **Context**: ADR-004(Connector), ADR-006(Worker 분리) 후속 결정 — 추론 서버 배포 책임과 통신 경계 명확화
> **Supersedes**: N/A

---

## TL;DR (한 줄 요약)

> 추론 서버 배포/관리는 **인프라 레벨**의 책임이고, Worker는 **이미 배포된 서버와 통신**하는 역할만 담당한다. 백엔드 선택(vLLM, Triton 등)은 상황과 모델에 따라 결정되므로 아키텍처 레벨에서 강제하지 않는다.

---

## 1. 문제 정의: 왜 이 결정이 필요한가

### 1.1 ADR-004/006의 미결정 영역

ADR-004(Connector)와 ADR-006(Worker 분리)은 각각 다음을 결정했다:

```
┌─────────────────────────────────────────────────────────────────────────┐
│                   기존 ADR에서 결정된 것                                  │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   [ADR-004: Connector Pattern]                                           │
│   ────────────────────────────                                           │
│   결정: 추론 서버와 통신 추상화                                          │
│   • 프로토콜 차이 추상화 (REST, gRPC, WebSocket)                        │
│   • 응답 포맷 정규화                                                    │
│                                                                          │
│   가정: "이미 떠있는 추론 서버"가 존재                                   │
│                                                                          │
│   ─────────────────────────────────────────────────────────────────      │
│                                                                          │
│   [ADR-006: Worker 리소스 분리]                                          │
│   ───────────────────────────────                                        │
│   결정: Task Queue 기반 CPU/Accelerator Worker 분리                     │
│   • CPU Worker: 전처리/후처리                                           │
│   • Accelerator Worker: 추론 Activity 실행                              │
│                                                                          │
│   가정: "Accelerator Worker가 추론 서버와 통신"                          │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

**미결정 영역:**

```
┌─────────────────────────────────────────────────────────────────────────┐
│                   기존 ADR에서 결정되지 않은 것                           │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   ❓ 추론 서버를 누가, 어디서 배포/관리하는가?                            │
│   ❓ 모델별로 어떤 백엔드(vLLM, Triton 등)를 선택하는가?                  │
│   ❓ Worker와 추론 서버의 배포 관계는? (통합 vs 분리)                    │
│   ❓ 백엔드별 설정/튜닝은 어디서 관리하는가?                              │
│                                                                          │
│   이 미결정 영역이 "범용 Connector" 가정의 허점이었다.                   │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### 1.2 실제 복잡성: 백엔드마다 다른 배포 방식

"추론 서버"라고 해도 백엔드마다 배포 방식이 완전히 다르다:

```
┌─────────────────────────────────────────────────────────────────────────┐
│                   백엔드별 배포 복잡성                                    │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   각 백엔드는 고유한 설정 체계, 모델 로드 방식, 최적화 전략을 가진다.    │
│   이 복잡성은 "통신 계층"이 아닌 "배포/운영 계층"에서 처리해야 한다.     │
│                                                                          │
│   ─────────────────────────────────────────────────────────────────      │
│                                                                          │
│   예시 (구체적 명령어는 참고용):                                         │
│                                                                          │
│   • vLLM: Python 기반, HuggingFace 통합, tensor-parallel 지원           │
│   • Triton: C++ 기반, Model Repository 구조, Dynamic Batching           │
│   • TGI: Rust 기반, 자체 샤딩 전략, 별도 파라미터 체계                  │
│   • SGLang: Python 기반, 별도 최적화 전략                                │
│                                                                          │
│   → 이 다양성을 Worker가 알 필요는 없다.                                 │
│   → Worker는 "어디로 요청을 보내면 되는지"만 알면 된다.                  │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### 1.3 핵심 문제

| 현상 (Symptom) | 본질 (Root Cause) |
|---------------|-------------------|
| "범용 Connector"가 모든 것을 해결할 것 같은 착각 | Connector는 통신만 담당, 배포는 별개 영역 |
| Worker가 백엔드별 특성을 알아야 할 것 같은 우려 | 책임 경계가 불명확 |
| 추론 서버 배포 책임이 모호 | 아키텍처에서 명시적으로 결정하지 않음 |

---

## 2. 전략적 결정

### 2.1 결정 #1: 추론 서버 배포/관리는 인프라 레벨 책임

> **"추론 서버의 배포, 설정, 스케일링은 컨테이너/오케스트레이션 인프라의 책임이다. 우리 애플리케이션(Worker)은 이미 배포된 서버와 통신만 한다."**

```
┌─────────────────────────────────────────────────────────────────────────┐
│                   책임 경계 정의                                          │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   ┌─────────────────────────────────────────────────────────────────┐   │
│   │  인프라 레벨 책임 (컨테이너 오케스트레이션)                      │   │
│   │  ─────────────────────────────────────────                       │   │
│   │                                                                   │   │
│   │  • 추론 서버 컨테이너 이미지 관리                                │   │
│   │  • 백엔드별 설정 (환경변수, ConfigMap 등)                       │   │
│   │  • GPU/가속기 리소스 할당                                        │   │
│   │  • 모델 로드 (저장소 마운트, 초기화)                            │   │
│   │  • 스케일링 (HPA, VPA, 수동)                                    │   │
│   │  • 헬스체크 및 재시작                                            │   │
│   │                                                                   │   │
│   │  도구 예시: Kubernetes, Docker Compose, KServe, Seldon Core     │   │
│   │  (구체적 도구 선택은 운영 환경에 따라 결정)                      │   │
│   │                                                                   │   │
│   └─────────────────────────────────────────────────────────────────┘   │
│                                                                          │
│   ┌─────────────────────────────────────────────────────────────────┐   │
│   │  애플리케이션 레벨 책임 (Worker/Connector)                       │   │
│   │  ───────────────────────────────────────────                     │   │
│   │                                                                   │   │
│   │  • 추론 서버 엔드포인트 주소 인지                                │   │
│   │  • 프로토콜에 맞는 요청 생성                                     │   │
│   │  • 응답 수신 및 정규화                                           │   │
│   │  • 통신 오류 시 상위(Temporal)로 전파                            │   │
│   │                                                                   │   │
│   │  Worker가 알아야 할 것: "어디로 요청을 보내면 되는가"            │   │
│   │  Worker가 몰라도 되는 것: "그 서버가 어떻게 배포되었는가"        │   │
│   │                                                                   │   │
│   └─────────────────────────────────────────────────────────────────┘   │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

**이 분리의 이점:**

| 이점 | 설명 |
|------|------|
| **관심사 분리** | Worker 개발자는 추론 서버 인프라를 몰라도 됨 |
| **독립적 스케일링** | Worker와 추론 서버를 개별적으로 스케일링 가능 |
| **유연한 교체** | 추론 서버 백엔드 교체 시 Worker 코드 변경 불필요 |
| **전문성 분리** | 인프라팀/ML팀의 책임 명확화 |

---

### 2.2 결정 #2: 백엔드 선택은 아키텍처에서 강제하지 않음

> **"어떤 추론 백엔드(vLLM, Triton, TGI 등)를 사용할지는 모델 특성, 요구사항, 팀 역량에 따라 결정한다. 아키텍처 레벨에서 특정 백엔드를 강제하지 않는다."**

```
┌─────────────────────────────────────────────────────────────────────────┐
│                   백엔드 선택 원칙                                        │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   ❌ 하지 않는 것:                                                       │
│   ────────────────                                                       │
│   • "LLM은 반드시 vLLM 사용" 같은 강제                                  │
│   • "CV 모델은 반드시 Triton 사용" 같은 강제                            │
│   • 특정 백엔드를 "표준"으로 지정                                       │
│                                                                          │
│   ✅ 하는 것:                                                            │
│   ──────────                                                             │
│   • 모델/프로젝트별로 적합한 백엔드 선택 허용                           │
│   • 선택 시 고려 요소 가이드 제공 (설계/운영 문서에서)                  │
│   • Connector가 다양한 백엔드와 통신할 수 있도록 확장성 보장            │
│                                                                          │
│   ─────────────────────────────────────────────────────────────────      │
│                                                                          │
│   백엔드 선택 시 고려 요소 (가이드, 강제 아님):                          │
│   ────────────────────────────────────────────                           │
│                                                                          │
│   • 모델 타입: LLM, Embedding, Vision, Audio 등                         │
│   • 스트리밍 필요 여부                                                   │
│   • 배치 처리 요구사항                                                   │
│   • 팀의 기존 경험과 역량                                                │
│   • 하드웨어 호환성                                                      │
│   • 커뮤니티/지원 상태                                                   │
│                                                                          │
│   (구체적 선택 가이드는 운영/설계 문서에서 제공)                        │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

**이 원칙의 이유:**

1. **기술 변화 대응**: 추론 백엔드 생태계는 빠르게 진화함. 특정 백엔드에 종속되면 새로운 최적 선택지를 놓칠 수 있음
2. **다양한 요구사항**: 같은 조직 내에서도 모델별로 최적 백엔드가 다를 수 있음
3. **Backend Agnostic 원칙 유지**: ADR-004의 핵심 원칙 준수

---

### 2.3 결정 #3: Worker-Server 분리 배포

> **"Worker와 추론 서버는 분리 배포한다. 둘은 네트워크를 통해 통신하며, 배포 라이프사이클이 독립적이다."**

```
┌─────────────────────────────────────────────────────────────────────────┐
│                   분리 배포 모델                                          │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   ┌───────────────────────────────────────────────────────────────┐     │
│   │  Worker Pool                                                   │     │
│   │  ───────────                                                   │     │
│   │                                                                 │     │
│   │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐            │     │
│   │  │ CPU Worker  │  │ CPU Worker  │  │ Accel Worker│            │     │
│   │  │     #1      │  │     #2      │  │     #1      │            │     │
│   │  └──────┬──────┘  └──────┬──────┘  └──────┬──────┘            │     │
│   │         │                │                │                    │     │
│   └─────────┼────────────────┼────────────────┼────────────────────┘     │
│             │                │                │                          │
│             │         HTTP / gRPC             │                          │
│             │                │                │                          │
│   ┌─────────┼────────────────┼────────────────┼────────────────────┐     │
│   │         ▼                ▼                ▼                    │     │
│   │  Inference Server Pool                                         │     │
│   │  ─────────────────────                                         │     │
│   │                                                                 │     │
│   │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐            │     │
│   │  │ vLLM Server │  │ Triton      │  │ TGI Server  │            │     │
│   │  │ (Llama-70B) │  │ (Whisper)   │  │ (Mistral)   │            │     │
│   │  │ GPU: 4x A100│  │ GPU: 1x L40 │  │ GPU: 2x A100│            │     │
│   │  └─────────────┘  └─────────────┘  └─────────────┘            │     │
│   │                                                                 │     │
│   └─────────────────────────────────────────────────────────────────┘     │
│                                                                          │
│   배포 관계: N:M (Worker N개, Server M개)                               │
│   통신 방식: 네트워크 (HTTP/gRPC)                                       │
│   라이프사이클: 독립적                                                   │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

**분리 배포의 이점:**

| 이점 | 설명 |
|------|------|
| **독립적 스케일링** | Worker 부하와 추론 부하를 개별적으로 스케일링 |
| **장애 격리** | 추론 서버 장애가 Worker 전체에 영향 주지 않음 |
| **리소스 최적화** | 추론 서버는 GPU 노드에, Worker는 CPU 노드에 배치 |
| **유연한 업데이트** | 추론 서버 업데이트 시 Worker 재배포 불필요 |

**대안으로 고려했으나 기각한 것:**

```
┌─────────────────────────────────────────────────────────────────────────┐
│                   기각된 대안: Sidecar 패턴                               │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   [Worker + Sidecar 통합 배포]                                          │
│                                                                          │
│   ┌─────────────────────────────────────────────────────────────────┐   │
│   │  Pod                                                             │   │
│   │  ┌─────────────┐  ┌─────────────┐                               │   │
│   │  │   Worker    │  │  Inference  │                               │   │
│   │  │             │◀─│  Sidecar    │                               │   │
│   │  │             │  │  (vLLM)     │                               │   │
│   │  └─────────────┘  └─────────────┘                               │   │
│   │                     GPU 점유                                     │   │
│   └─────────────────────────────────────────────────────────────────┘   │
│                                                                          │
│   기각 사유:                                                             │
│   ──────────                                                             │
│   • Worker가 유휴 상태여도 GPU 계속 점유 → 리소스 낭비                  │
│   • Worker와 추론 서버 스케일링 비율이 같아야 함 → 유연성 감소          │
│   • GPU 노드에 Worker가 강제 배치 → 비용 증가                           │
│                                                                          │
│   예외적 허용:                                                           │
│   ─────────                                                              │
│   • 네트워크 지연이 절대 허용 불가한 특수 케이스                        │
│   • 이 경우 설계 문서에서 별도 결정                                     │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## 3. 아키텍처 개요

### 3.1 전체 배포 구조

```
┌─────────────────────────────────────────────────────────────────────────┐
│                   배포 아키텍처 개요                                      │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   ┌─────────────────────────────────────────────────────────────────┐   │
│   │  인프라 관리 영역                                                │   │
│   │  ─────────────────                                               │   │
│   │                                                                   │   │
│   │  ┌───────────────────────────────────────────────────────────┐  │   │
│   │  │  Inference Server Deployment                               │  │   │
│   │  │  ────────────────────────────                               │  │   │
│   │  │                                                             │  │   │
│   │  │  • 컨테이너 이미지 관리                                     │  │   │
│   │  │  • 백엔드별 설정 (ConfigMap, Secret)                       │  │   │
│   │  │  • GPU 리소스 할당                                          │  │   │
│   │  │  • 모델 저장소 마운트                                       │  │   │
│   │  │  • 서비스 엔드포인트 노출                                   │  │   │
│   │  │                                                             │  │   │
│   │  │  결과물: http://llama-vllm:8000, grpc://whisper-triton:8001 │  │   │
│   │  └───────────────────────────────────────────────────────────┘  │   │
│   │                                                                   │   │
│   └─────────────────────────────────────────────────────────────────┘   │
│                                                                          │
│                              │                                           │
│                              │ 엔드포인트 정보 전달                      │
│                              │ (환경변수, ConfigMap, Service Discovery) │
│                              ▼                                           │
│                                                                          │
│   ┌─────────────────────────────────────────────────────────────────┐   │
│   │  애플리케이션 영역                                               │   │
│   │  ─────────────────                                               │   │
│   │                                                                   │   │
│   │  ┌───────────────────────────────────────────────────────────┐  │   │
│   │  │  Worker                                                     │  │   │
│   │  │  ──────                                                     │  │   │
│   │  │                                                             │  │   │
│   │  │  Activity 코드:                                             │  │   │
│   │  │  • 추론 서버 엔드포인트 획득 (설정 또는 Service Discovery) │  │   │
│   │  │  • Connector로 요청 전송                                    │  │   │
│   │  │  • 응답 처리                                                │  │   │
│   │  │                                                             │  │   │
│   │  │  Worker가 모르는 것:                                        │  │   │
│   │  │  • 추론 서버가 vLLM인지 Triton인지 (Connector가 처리)      │  │   │
│   │  │  • 서버가 GPU 몇 개로 실행 중인지                           │  │   │
│   │  │  • 모델이 어떻게 로드되었는지                               │  │   │
│   │  └───────────────────────────────────────────────────────────┘  │   │
│   │                                                                   │   │
│   └─────────────────────────────────────────────────────────────────┘   │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### 3.2 엔드포인트 디스커버리

Worker가 추론 서버 엔드포인트를 알아내는 방법:

| 방식 | 설명 | 적용 시점 |
|------|------|----------|
| **환경변수** | `LLAMA_ENDPOINT=http://llama-vllm:8000` | 단순 배포 |
| **ConfigMap** | 모델-엔드포인트 매핑 설정 | 다중 모델 |
| **Service Discovery** | K8s Service, Consul 등 | 동적 스케일링 |

> **원칙**: 엔드포인트 디스커버리 방식은 배포 환경에 따라 결정한다. 아키텍처에서 특정 방식을 강제하지 않는다.

---

## 4. ADR 체계 내 위치

```
┌─────────────────────────────────────────────────────────────────────────┐
│                   ADR 의존 관계 (업데이트)                                │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   ADR-001: Edge Layer (Nginx)                                           │
│      │                                                                   │
│      ▼                                                                   │
│   ADR-002: Pipeline Manager                                              │
│      │                                                                   │
│      ▼                                                                   │
│   ADR-003: Execution Model (Temporal)                                    │
│      │                                                                   │
│      ├───────────────────────────────────────┐                          │
│      ▼                                       ▼                          │
│   ADR-004: Connector                     ADR-006: Worker 분리           │
│      │    통신 추상화                        │    리소스 분리            │
│      │                                       │                          │
│      └───────────────┬───────────────────────┘                          │
│                      │                                                   │
│                      ▼                                                   │
│   ADR-007: 추론 서버 배포 경계 (본 문서)                                │
│      │    ─────────────────────────────                                  │
│      │    • 배포 책임: 인프라 레벨                                       │
│      │    • 통신 책임: Worker/Connector                                  │
│      │    • 배포 관계: 분리                                              │
│      │                                                                   │
│      └───▶ 인프라 설계/운영 문서로 연결                                 │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## 5. 트레이드오프 및 제약

### 5.1 감수하는 단점

| 단점 | 영향 | 보완 전략 |
|------|------|----------|
| **네트워크 오버헤드** | Worker-Server 간 네트워크 통신 필요 | 같은 클러스터/노드 그룹 배치로 지연 최소화 |
| **배포 복잡성** | Worker와 Server 개별 관리 필요 | 인프라 자동화 (Helm, Operator) |
| **엔드포인트 관리** | 다중 서버 엔드포인트 관리 필요 | Service Discovery 또는 설정 중앙화 |

### 5.2 재검토 트리거

다음 상황 발생 시 이 결정을 재검토한다:

- [ ] 네트워크 지연이 SLA를 충족하지 못하는 워크로드 발생
- [ ] Worker와 추론 서버의 라이프사이클 통합이 더 효율적인 시나리오 발견
- [ ] Sidecar 패턴의 리소스 낭비 문제가 기술적으로 해결된 경우

---

## 6. 결론

### 6.1 최종 결정 요약

| 결정 항목 | 결정 내용 | 핵심 논거 |
|----------|----------|----------|
| **배포 책임** | 인프라 레벨 | 관심사 분리, Worker 단순화 |
| **백엔드 선택** | 아키텍처에서 강제하지 않음 | 상황/모델별 최적 선택 허용 |
| **배포 관계** | Worker-Server 분리 배포 | 독립적 스케일링, 리소스 최적화 |

### 6.2 핵심 메시지

> **"추론 서버의 배포와 운영은 인프라의 책임이다. Worker는 '어떤 서버가 어떻게 배포되었는지' 몰라도 된다. 엔드포인트만 알면 Connector를 통해 통신할 수 있다. 이 분리가 시스템의 유연성과 확장성을 보장한다."**

---

## 관련 문서 (Related)

- [ADR-004: Universal Connector](./004_Universal_Connector_Strategy_for_Model_Integration.md) — 통신 추상화
- [ADR-006: Worker 리소스 분리](./006_Worker_Resource_Separation_and_Parallel_Processing_Strategy.md) — Worker 분리 전략
- 02_architecture/04_worker.md — Worker 상세 설계
- 04_operations/02_deployment.md — 배포 운영 가이드 (작성 예정)

---

## 변경 이력 (Changelog)

| 날짜 | 작성자 | 변경 내용 |
|------|--------|----------|
| 2024-12-27 | System Architect | 초안 작성 — 추론 서버 배포 책임, 백엔드 선택 원칙, 분리 배포 결정 |
